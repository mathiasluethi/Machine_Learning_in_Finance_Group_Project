{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Learning, UZH FS18, Group Project\n",
    "\n",
    "### Group 2: Barbara Capl, Mathias Lüthi, Pamela Matias, Stefanie Rentsch\n",
    "\n",
    "\n",
    "#     \n",
    "# III.     Feature Exraction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide unnecessary warnings (\"depreciation\" of packages etc.)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import datetime as dt\n",
    "import sklearn as skl\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neural_network\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import neighbors\n",
    "from functools import reduce\n",
    "from functools import reduce\n",
    "from pandas.core import datetools\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Imputed Dataset = (3519, 94)\n",
      "Shape of Dataset with Nans dropped = (1430, 94)\n"
     ]
    }
   ],
   "source": [
    "# Import imputed dataset\n",
    "imputed_dataset = pd.read_csv('Data/generated/imputed_dataset_ml.csv', sep = ',')\n",
    "# Import dataset wit dropped Nans\n",
    "dropnan_dataset = pd.read_csv('Data/generated/dropnan_dataset_ml.csv', sep = ',')\n",
    "\n",
    "print('Shape of Imputed Dataset = ' + str(imputed_dataset.shape))\n",
    "print('Shape of Dataset with Nans dropped = ' + str(dropnan_dataset.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Version 1: Imputed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Matrix and Response Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>BIDLO</th>\n",
       "      <th>ASKHI</th>\n",
       "      <th>PRC</th>\n",
       "      <th>VOL</th>\n",
       "      <th>BID</th>\n",
       "      <th>ASK</th>\n",
       "      <th>...</th>\n",
       "      <th>sale_nwc</th>\n",
       "      <th>rd_sale</th>\n",
       "      <th>adv_sale</th>\n",
       "      <th>staff_sale</th>\n",
       "      <th>accrual</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>divyield</th>\n",
       "      <th>PEG_1yrforward</th>\n",
       "      <th>PEG_ltgforward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10107.0</td>\n",
       "      <td>1.138752e+18</td>\n",
       "      <td>511210.0</td>\n",
       "      <td>26.39</td>\n",
       "      <td>28.0400</td>\n",
       "      <td>26.87</td>\n",
       "      <td>11088149.0</td>\n",
       "      <td>26.87</td>\n",
       "      <td>26.88</td>\n",
       "      <td>...</td>\n",
       "      <td>1.323</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036</td>\n",
       "      <td>6.281</td>\n",
       "      <td>10.280</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>14.555</td>\n",
       "      <td>1.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10107.0</td>\n",
       "      <td>1.141171e+18</td>\n",
       "      <td>511210.0</td>\n",
       "      <td>26.85</td>\n",
       "      <td>27.8900</td>\n",
       "      <td>27.21</td>\n",
       "      <td>14514337.0</td>\n",
       "      <td>27.24</td>\n",
       "      <td>27.24</td>\n",
       "      <td>...</td>\n",
       "      <td>1.323</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036</td>\n",
       "      <td>6.293</td>\n",
       "      <td>10.410</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>14.739</td>\n",
       "      <td>1.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10107.0</td>\n",
       "      <td>1.143850e+18</td>\n",
       "      <td>511210.0</td>\n",
       "      <td>24.15</td>\n",
       "      <td>27.7400</td>\n",
       "      <td>24.15</td>\n",
       "      <td>14689919.0</td>\n",
       "      <td>24.16</td>\n",
       "      <td>24.16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.323</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036</td>\n",
       "      <td>5.573</td>\n",
       "      <td>9.239</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>13.081</td>\n",
       "      <td>1.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10107.0</td>\n",
       "      <td>1.146442e+18</td>\n",
       "      <td>511210.0</td>\n",
       "      <td>22.56</td>\n",
       "      <td>24.2900</td>\n",
       "      <td>22.65</td>\n",
       "      <td>23651189.0</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.70</td>\n",
       "      <td>...</td>\n",
       "      <td>1.388</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024</td>\n",
       "      <td>5.496</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>-5.842</td>\n",
       "      <td>1.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10107.0</td>\n",
       "      <td>1.149120e+18</td>\n",
       "      <td>511210.0</td>\n",
       "      <td>21.51</td>\n",
       "      <td>23.4702</td>\n",
       "      <td>23.30</td>\n",
       "      <td>19980809.0</td>\n",
       "      <td>23.38</td>\n",
       "      <td>23.31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.388</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024</td>\n",
       "      <td>5.577</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>-6.010</td>\n",
       "      <td>1.522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   PERMNO          DATE     NAICS  BIDLO    ASKHI    PRC  \\\n",
       "0           1  10107.0  1.138752e+18  511210.0  26.39  28.0400  26.87   \n",
       "1           2  10107.0  1.141171e+18  511210.0  26.85  27.8900  27.21   \n",
       "2           3  10107.0  1.143850e+18  511210.0  24.15  27.7400  24.15   \n",
       "3           4  10107.0  1.146442e+18  511210.0  22.56  24.2900  22.65   \n",
       "4           5  10107.0  1.149120e+18  511210.0  21.51  23.4702  23.30   \n",
       "\n",
       "          VOL    BID    ASK       ...        sale_nwc  rd_sale  adv_sale  \\\n",
       "0  11088149.0  26.87  26.88       ...           1.323    0.151     0.025   \n",
       "1  14514337.0  27.24  27.24       ...           1.323    0.151     0.025   \n",
       "2  14689919.0  24.16  24.16       ...           1.323    0.151     0.025   \n",
       "3  23651189.0  22.70  22.70       ...           1.388    0.150     0.025   \n",
       "4  19980809.0  23.38  23.31       ...           1.388    0.150     0.025   \n",
       "\n",
       "   staff_sale  accrual    ptb  PEG_trailing  divyield  PEG_1yrforward  \\\n",
       "0         0.0    0.036  6.281        10.280    0.0134          14.555   \n",
       "1         0.0    0.036  6.293        10.410    0.0132          14.739   \n",
       "2         0.0    0.036  5.573         9.239    0.0149          13.081   \n",
       "3         0.0    0.024  5.496         0.709    0.0159          -5.842   \n",
       "4         0.0    0.024  5.577         0.730    0.0155          -6.010   \n",
       "\n",
       "   PEG_ltgforward  \n",
       "0           1.838  \n",
       "1           1.842  \n",
       "2           1.666  \n",
       "3           1.480  \n",
       "4           1.522  \n",
       "\n",
       "[5 rows x 93 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract labels of features\n",
    "labels_of_features_1 = imputed_dataset.columns[:-1]\n",
    "type(labels_of_features_1)\n",
    "\n",
    "# X1 is the feature matrix\n",
    "X1 = imputed_dataset.iloc[:, :-1]\n",
    "\n",
    "display(X1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "Name: NEXT_DAY_PREDICTION, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y1 is the response vector\n",
    "y1 = imputed_dataset.iloc[:, -1]\n",
    "display(y1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - / Test - Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the train - test- split\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.2, random_state = 0, stratify = y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5602131438721136"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5610795454545454"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if there is the approximately same percentage of '1' i both training and test response vector\n",
    "display(y1_train.sum() / y1_train.size)\n",
    "display(y1_test.sum() / y1_test.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization with sklearn StandardScaler\n",
    "standard_scaler_1 = preprocessing.StandardScaler().fit(X1_train)\n",
    "X1_train = standard_scaler_1.transform(X1_train)\n",
    "X1_test = standard_scaler_1.transform(X1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of all Ones (Train) = 0.5602131438721136\n",
      "Score (Prediction) =  0.5241477272727273\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE\n",
    "# # https://stats.stackexchange.com/questions/144439/applying-pca-to-test-data-for-classification-purposes\n",
    "\n",
    "\n",
    "# predict labels using the trained classifier \n",
    "\n",
    "pipe_1 = Pipeline([('pca', PCA(n_components = 1)),\n",
    "                 ('tree', RandomForestClassifier())])\n",
    "\n",
    "pipe_1.fit(X1_train, y1_train)\n",
    "\n",
    "prediction_1 = pipe_1.predict(X1_test)\n",
    "\n",
    "print('Sum of all Ones (Train) = ' + str(y1_train.sum() / y1_train.size))\n",
    "print('Score (Prediction) =  ' + str(prediction_1.sum() / prediction_1.size))\n",
    "print(\"\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   \n",
    "# Feature Extraction with Principal Component Analysis (PCA)\n",
    "###   \n",
    "## Feature Extraction \n",
    "## for Version 1: Imputed Dataset\n",
    "###   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PCA on whole Training Set for all possible PCAs (= number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA for all possible PCAs\n",
    "pca_a1 = PCA().fit(X1_train)\n",
    "\n",
    "# Define maximal number of principal components => the \"1\" in shape[1] refers to columns (\"0\" would be rows)\n",
    "q_a1 = X1_train.shape[1]\n",
    "\n",
    "# Get the amount of variance that each PC explains\n",
    "# The eigenvalues represent the variance in the direction of the eigenvector\n",
    "# These numbers for each component are proportional to the Eigenvalues \n",
    "# This means that the ratio of the eigenvalue of the first principal component \n",
    "# to the eigenvalue of the second principal component is 0.16214649\n",
    "# SEE => https://stackoverflow.com/questions/37757172/finding-and-utilizing-eigenvalues-and-eigenvectors-from-pca-in-scikit-learn?rpca.q=1\n",
    "expl_var_a1 = pca_a1.explained_variance_ratio_\n",
    "\n",
    "# Get cumulative sum of the PCA 1-q_a1\n",
    "sum_expl_var_a1 = np.cumsum(expl_var_a1)[:q_a1]\n",
    "\n",
    "# because we run PCA for all possible PCAs, sum of al explained Variance of the training set should be 1\n",
    "\n",
    "print(\"\")\n",
    "print('Explained Variance, first 10 rows: ')\n",
    "print(expl_var_a1[0:10])\n",
    "print(\"\")\n",
    "print('Explained Variance in Total = ' + str(expl_var_a1.sum()))\n",
    "print(\"\")\n",
    "print('Cumulative explained Variance, first 10 rows: ')\n",
    "print(sum_expl_var_a1[0:10])\n",
    "print(\"\")\n",
    "print('Maximal number (q_1) of PCs is: ' + str(q_a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot curve with cumulative sum\n",
    "plt.plot(sum_expl_var_a1)\n",
    "plt.title('Cumulative explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Ratio of Cum. explained Variance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot curve with explained variance\n",
    "plt.plot(expl_var_a1)\n",
    "plt.title('Explained Variance by single components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Ratio of Variance explained')\n",
    "plt.xticks(range(-1, q_a1 + 1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importances (both cumulative and idividual)\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.plot(expl_var_a1) #range(0, q_1 + 1), align = 'center')\n",
    "plt.xticks(range(0, q_a1 + 1, 1))\n",
    "plt.xlim([0, 30])\n",
    "plt.xlabel('Principal Components Version a1')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.step(range(1, q_a1 + 1), sum_expl_var_a1, where = 'mid')\n",
    "\n",
    "plt.tight_layout();\n",
    "\n",
    "########  =>>> plt.bar(range(0, q_1), expl_var_1, alogn = 'center') gives ERROR MESSAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose number of Principal Components  and get them for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of principal components we wish to extract\n",
    "q_1 = 10\n",
    "\n",
    "# Create PCA object\n",
    "pca_1 = PCA(n_components = q_1)\n",
    "\n",
    "# Fit PCA object to find first principal components\n",
    "pca_1.fit(X1_train)\n",
    "\n",
    "pca_1\n",
    "\n",
    "# Get the amount of variance that each PC explains\n",
    "# The eigenvalues represent the variance in the direction of the eigenvector\n",
    "# These numbers for each component are proportional to the Eigenvalues \n",
    "# This means that the ratio of the eigenvalue of the first principal component \n",
    "# to the eigenvalue of the second principal component is 0.16214649\n",
    "# SEE => https://stackoverflow.com/questions/37757172/finding-and-utilizing-eigenvalues-and-eigenvectors-from-pca-in-scikit-learn?rpca.q=1\n",
    "expl_var_1 = pca_1.explained_variance_ratio_\n",
    "\n",
    "# Get cumulative sum of the PCA 1-q_1\n",
    "sum_expl_var_1 = np.cumsum(expl_var_1)[:q_1]\n",
    "\n",
    "# because we run PCA for only q_1 components, sum of al explained Variance of the training set should be LESS than 1\n",
    "\n",
    "print(\"\")\n",
    "print('Explained Variance, first 10 rows: ')\n",
    "print(expl_var_1[0:10])\n",
    "print(\"\")\n",
    "print('Explained Variance in Total = ' + str(expl_var_1.sum()))\n",
    "print(\"\")\n",
    "print('Cumulative explained Variance, first 10 rows: ')\n",
    "print(sum_expl_var_1[0:10])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract q_1 number of features out of Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract q_1 number of features according to pca analysis\n",
    "# WEBSITE => https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/\n",
    "X1_train_transformed = pca_1.fit_transform(X1_train)\n",
    "display(X1_train_transformed)\n",
    "len(X1_train_transformed)\n",
    "\n",
    "# This gives the Eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform new data (Test set) using the already fitted pca_1\n",
    "##    \n",
    "## (((NOT SURE IT ITS CORRECT)))\n",
    "##   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA components\n",
    "\n",
    "### Dont know how to use this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print PCA components: every row is a principal component in the p-dimensional space\n",
    "# Principal axes in feature space, representing the directions of maximum variance in the data. \n",
    "# The components are sorted by explained_variance_ \n",
    "# SEE SKLEARN DOCUMENTATION\n",
    "\n",
    "print(pca_1.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X1_train_transformed = pca_1.fit_transform(X1_train)\n",
    "X1_test_transformed = pca_1.transform(X1_test)\n",
    "\n",
    "\n",
    "my_forest_1 = RandomForestClassifier(random_state = 1)\n",
    "my_forest_1.max_depth = 8\n",
    "my_forest_1.fit(X1_train_transformed, y1_train)\n",
    "\n",
    "prediction_1 = my_forest_1.predict(X1_test_transformed)\n",
    "\n",
    "display(prediction_1[1:5])\n",
    "\n",
    "\n",
    "print('Sum of all Ones (Train) = ' + str(y1_train.sum() / y1_train.size))\n",
    "print('Score (Prediction) =  ' + str(prediction_1.sum() / prediction_1.size))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Version 2: Dataset with rows dropped where Nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Matrix and Response Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels of features\n",
    "labels_of_features_2 = dropnan_dataset.columns[:-1]\n",
    "type(labels_of_features_2)\n",
    "\n",
    "# X2 is the feature matrix\n",
    "X2 = dropnan_dataset.iloc[:, :-1]\n",
    "\n",
    "display(X2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y2 is the response vector\n",
    "y2 = dropnan_dataset.iloc[:, -1]\n",
    "display(y2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - / Test - Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the train - test- split\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.2, random_state = 0, stratify = y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is the approximately same percentage of '1' i both training and test response vector\n",
    "display(y2_train.sum() / y2_train.size)\n",
    "display(y2_test.sum() / y2_test.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization with sklearn StandardScaler\n",
    "standard_scaler_2 = preprocessing.StandardScaler().fit(X2_train)\n",
    "X2_train = standard_scaler_2.transform(X2_train)\n",
    "X2_test = standard_scaler_2.transform(X2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   \n",
    "## Feature Extraction \n",
    "## for Version 2: Dataset with rows dropped where Nan\n",
    "###   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PCA on whole Training Set for all possible PCAs (= number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA for all possible PCAs\n",
    "pca_2 = PCA().fit(X2_train)\n",
    "\n",
    "# Define maximal number of principal components => the \"1\" in shape[1] refers to columns (\"0\" would be rows)\n",
    "q_2 = X2_train.shape[1]\n",
    "\n",
    "# Get the amount of variance that each PC explains\n",
    "expl_var_2 = pca_2.explained_variance_ratio_\n",
    "\n",
    "# Get cumulative sum of the PCA 1-q\n",
    "sum_expl_var_2 = np.cumsum(expl_var_2)[:q_2]\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print('Explained Variance, first 10 rows: ')\n",
    "print(expl_var_2[0:10])\n",
    "print(\"\")\n",
    "print('Explained Variance in Total = ' + str(expl_var_2.sum()))\n",
    "print(\"\")\n",
    "print('Cumulative explained Variance, first 10 rows: ')\n",
    "print(sum_expl_var_2[0:10])\n",
    "print(\"\")\n",
    "print('Maximal number (q_2) of PCs is: ' + str(q_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot curve with cumulative sum\n",
    "plt.plot(sum_expl_var_2)\n",
    "plt.title('Cumulative explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Ratio of Cum. explained Variance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot curve with explained variance\n",
    "plt.plot(expl_var_1)\n",
    "plt.title('Explained Variance by single components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Ratio of Variance explained')\n",
    "plt.xticks(range(-1, q_1 + 1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract q_1 number of features according to pca analysis\n",
    "# WEBSITE => https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/\n",
    "X2_train_extracted = pca_2.fit_transform(X2_train)\n",
    "\n",
    "len(X2_train_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importances (both cumulative and idividual)\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.plot(expl_var_2) #range(0, q_2 + 1), align = 'center')\n",
    "plt.xticks(range(0, q_2 + 1, 1))\n",
    "plt.xlim([0, 30])\n",
    "plt.xlabel('Principal Components Version 2')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.step(range(1, q_2 + 1), sum_expl_var_2, where = 'mid')\n",
    "\n",
    "plt.tight_layout();\n",
    "\n",
    "########  =>>> plt.bar(range(0, q_2), expl_var_2, alogn = 'center') gives ERROR MESSAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose number of Principal Components  and get them for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of principal components we wish to extract\n",
    "q_2 = 15\n",
    "\n",
    "# Create PCA object\n",
    "pca_2 = PCA(n_components = q_2)\n",
    "\n",
    "# Fit PCA object to find first principal components\n",
    "pca_2.fit(X2_train)\n",
    "\n",
    "pca_2\n",
    "\n",
    "# Get the amount of variance that each PC explains\n",
    "# The eigenvalues represent the variance in the direction of the eigenvector\n",
    "# These numbers for each component are proportional to the Eigenvalues \n",
    "# This means that the ratio of the eigenvalue of the first principal component \n",
    "# to the eigenvalue of the second principal component is 0.16214649\n",
    "# SEE => https://stackoverflow.com/questions/37757172/finding-and-utilizing-eigenvalues-and-eigenvectors-from-pca-in-scikit-learn?rpca.q=1\n",
    "expl_var_2 = pca_2.explained_variance_ratio_\n",
    "\n",
    "# Get cumulative sum of the PCA 1-q_2\n",
    "sum_expl_var_2 = np.cumsum(expl_var_2)[:q_2]\n",
    "\n",
    "# because we run PCA for only q_2 components, sum of al explained Variance of the training set should be LESS than 1\n",
    "\n",
    "print(\"\")\n",
    "print('Explained Variance, first 10 rows: ')\n",
    "print(expl_var_2[0:10])\n",
    "print(\"\")\n",
    "print('Explained Variance in Total = ' + str(expl_var_2.sum()))\n",
    "print(\"\")\n",
    "print('Cumulative explained Variance, first 10 rows: ')\n",
    "print(sum_expl_var_2[0:10])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract q_2 number of features out of Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract q_2 number of features according to pca analysis\n",
    "# WEBSITE => https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/\n",
    "X2_train_extracted = pca_2.fit_transform(X2_train)\n",
    "display(X2_train_extracted)\n",
    "len(X2_train_extracted)\n",
    "\n",
    "# This gives the Eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform new data (Test set) using the already fitted pca_2\n",
    "##    \n",
    "## (((NOT SURE IT ITS CORRECT)))\n",
    "##   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fitted pca_1 on another dataset, lilke X_test to transform it\n",
    "X2_test_transformed = pca_2.transform(X2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA components\n",
    "\n",
    "### Dont know how to use this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print PCA components: every row is a principal component in the p-dimensional space\n",
    "# Principal axes in feature space, representing the directions of maximum variance in the data. \n",
    "# The components are sorted by explained_variance_ \n",
    "# SEE SKLEARN DOCUMENTATION\n",
    "\n",
    "print(pca_2.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines and Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/144439/applying-pca-to-test-data-for-classification-purposes\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
